{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7618bc9a-9127-4b64-a444-99da4d77fe90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Running Our Auto Loader Bronze Layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17db5b05-a557-41d0-b50b-4176b847f683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assigning variables with our databricks file share paths that we created previously in 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c78ce3f4-133c-4bb7-ab5d-f5ae6fcda196",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%Python # use if you have your default set to another language\n",
    "\n",
    "source_data_loc = \"dbfs:/FileStore/tables/TraderJoesRevenue/Files/\"\n",
    "target_data_loc = \"dbfs:/FileStore/tables/TraderJoesRevenue/bronze_tj_fact_revenue\"\n",
    "checkpoints_loc = \"dbfs:/FileStore/tables/TraderJoesRevenue/checkpoints/\"\n",
    "schema_loc = \"dbfs:/FileStore/tables/TraderJoesRevenue/schema/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023a7257-0d95-433c-a36e-8f571b0eed3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading the cloud files that are stored in our DBFS using structured streaming (Autoloader).\n",
    "\n",
    "The file watch location can be changed to an Amazon S3 Bucket or Azure FileShare or Blob Storage location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cadc7681-4eae-43e6-a43f-415c2362a29a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fb4d8651110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%Python # use if you have your default set to another language\n",
    "\n",
    "df=(spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"csv\") \\\n",
    "  .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", schema_loc) \\\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n",
    "  .load(source_data_loc) \\\n",
    "  )\n",
    "\n",
    "df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .option(\"checkpointLocation\", checkpoints_loc) \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .trigger(availableNow=True) \\\n",
    "  .start(target_data_loc)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3309121777485545,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Bronze Fact Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
